# ðŸ SU_ResearchTask05_IndianCricketTeamAnalysis

This research project explores career statistics of Indian national cricket team players and captains (ODI & Test) using structured datasets.  
It is part of the OPT Research Task 05 at Syracuse University.

---

## ðŸ“¦ Dataset

We use 4 CSV files:
- `AllOdiPlayers.csv` â€“ 241 rows Ã— 10 columns  
- `AllOdicaptains.csv` â€“ 25 rows Ã— 8 columns  
- `AllTestplayers.csv` â€“ 303 rows Ã— 10 columns  
- `AllTestcaptains.csv` â€“ 31 rows Ã— 7 columns  

**Source**: [Wikipedia](https://en.wikipedia.org/)

> âš ï¸ **These files are not committed to the repo**. Please download them and place under `/data`.

---

## ðŸ§  Research Goal

The objective is to evaluate how well LLMs (like GPT-4) can answer questions using this dataset.  
We write natural language prompts and compare LLM outputs to actual values computed via Python.

Key research questions:
- Can the LLM answer direct stats queries reliably?
- Where does it hallucinate or approximate?
- How much reasoning does it apply?
- Can prompt engineering improve its accuracy?

---

## ðŸ“‚ Project Structure

```
SU_ResearchTask05_IndianCricketTeamAnalysis/
â”œâ”€â”€ data/                      # CSV files (ignored in Git)
â”œâ”€â”€ prompts/                  # Natural language questions + LLM answers
â”œâ”€â”€ validation/               # Python scripts to compute actual results
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ðŸ’¬ Prompt & Validation Flow

Each prompt follows this flow:

1. **Markdown File** (in `/prompts`)  
   - Contains the prompt question  
   - Captures the LLM response  
   - Logs the Python validation result

2. **Validation Script** (in `/validation`)  
   - Loads the CSVs  
   - Computes the actual answer  
   - Compares to LLMâ€™s output

### Example:
- `01_top_runs.md`: "Who scored the most ODI runs?"
- `01_top_runs.py`: Python to get the real answer from `AllOdiPlayers.csv`

---

## ðŸ“Š Visualizations

Some answers (e.g. top captains by win %) are also visualized using `matplotlib`.  
Example script: `08_best_captain_min20_visual.py`

To run charts:

```bash
pip install pandas matplotlib
```

---

## âœï¸ Reflections

This task made it clear that:
- **LLMs handle simple stats well** but need help on multi-condition filters
- Prompt phrasing plays a huge role
- Combining LLMs with Python validation ensures factual correctness
- This hybrid approach is ideal for research and dashboard automation

---

## ðŸ”„ Week 1â€“2: Progress Summary

During the first two weeks of Task 05, we focused on verifying how accurately an LLM (ChatGPT 4) could answer cricket-related statistical queries using structured CSV datasets of Indian ODI players and captains.

We designed and tested **9 core prompts** including:

- Top run scorer  
- Best batting average  
- Most matches played  
- Highest individual score  
- Most hundreds  
- Most runs as captain  
- Best win percentage (overall)  
- Best captain with 20+ matches  
- Visualization of top 5 captains by win %

For each question:
- We recorded the LLMâ€™s natural language response
- We validated it using custom Python scripts (in `/validation`)
- In some cases, we plotted results with Matplotlib

The LLM performed **well on direct queries** but required guidance or clarification for:
- Edge cases (e.g., single-match captains with 100% win)
- Ambiguities in columns or formatting

This dual approach proved effective in spotting LLM errors and helped us design better prompts for future experiments.

> All results are recorded in the `/prompts/` folder  
> All validation scripts are available in `/validation/`

---

## ðŸ“Œ Author

**Abhi Chakraborty**  
M.S. Applied Data Science, Syracuse University  
GitHub: [abhichkrbrty](https://github.com/abhichkrbrty/SU_ResearchTask05_IndianCricketTeamAnalysis)